{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUqmZXYA4YRX53SKWbYwgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milhaud1201/PyTorch_Tutorials/blob/main/Deep%20Learning%20Zero%20To%20All%20/Part_2_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab-08-1 Perceptron"
      ],
      "metadata": {
        "id": "Z3XIMiIKT3ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습목표\n",
        "\n",
        "퍼셉트론(Perceptron) 에 대해 알아본다."
      ],
      "metadata": {
        "id": "TEx3-jBjT6Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 핵심키워드\n",
        "\n",
        "퍼셉트론(Perceptron)  \n",
        "선형분류기(Linear Classifier)  \n",
        "AND, OR, XOR 게이트"
      ],
      "metadata": {
        "id": "1kRVILbjT9UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## xor\n",
        "\n"
      ],
      "metadata": {
        "id": "VhbWfnqWUKYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "hTYVg9fFW_JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rby9wX6mTwIC"
      },
      "outputs": [],
      "source": [
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device='cuda')\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## nn Layers\n",
        "linear = torch.nn.Linear(2, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear, sigmoid).to(device='cuda')\n",
        "\n",
        "# define cost/Loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device='cuda')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "for step in range(1001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/Loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-auPVHaW8N9",
        "outputId": "00d6f124-43c9-42df-acfa-f76576b4be58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.7083841562271118\n",
            "100 0.6931483745574951\n",
            "200 0.6931471824645996\n",
            "300 0.6931472420692444\n",
            "400 0.6931471824645996\n",
            "500 0.6931471824645996\n",
            "600 0.6931471824645996\n",
            "700 0.6931471824645996\n",
            "800 0.6931471824645996\n",
            "900 0.6931471824645996\n",
            "1000 0.6931471824645996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab-08-2 Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "ZbdfPPzdZbRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습목표\n",
        "\n",
        "다중 퍼셉트론(Multi Layer Perceptron) 에 대해 알아본다."
      ],
      "metadata": {
        "id": "bU7d2afmZc2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 핵심키워드\n",
        "\n",
        "다중 퍼셉트론(Multi Layer Perceptron)  \n",
        "오차역전파(Backpropagation)"
      ],
      "metadata": {
        "id": "FLwQ1lkvZhAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code: xor-nn"
      ],
      "metadata": {
        "id": "DQqWm4Kuru97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device='cuda')\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device='cuda')\n",
        "\n",
        "## nn Layers\n",
        "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
        "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device='cuda')\n",
        "\n",
        "# define cost/Loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device='cuda')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "for step in range(10001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/Loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxryNu6QZQET",
        "outputId": "e43dc16a-d5db-44b3-b8c6-4618fc78ae20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.7522538900375366\n",
            "100 0.6931231617927551\n",
            "200 0.6929280161857605\n",
            "300 0.6924945712089539\n",
            "400 0.6908154487609863\n",
            "500 0.6775641441345215\n",
            "600 0.5852385759353638\n",
            "700 0.46616268157958984\n",
            "800 0.4035629332065582\n",
            "900 0.3795297145843506\n",
            "1000 0.36884215474128723\n",
            "1100 0.36312833428382874\n",
            "1200 0.3596494197845459\n",
            "1300 0.3573334813117981\n",
            "1400 0.35569053888320923\n",
            "1500 0.35446906089782715\n",
            "1600 0.3535274863243103\n",
            "1700 0.35278084874153137\n",
            "1800 0.3521749973297119\n",
            "1900 0.35167402029037476\n",
            "2000 0.3512531518936157\n",
            "2100 0.35089489817619324\n",
            "2200 0.35058632493019104\n",
            "2300 0.35031789541244507\n",
            "2400 0.3500823974609375\n",
            "2500 0.34987401962280273\n",
            "2600 0.3496885895729065\n",
            "2700 0.34952235221862793\n",
            "2800 0.34937262535095215\n",
            "2900 0.3492370843887329\n",
            "3000 0.34911373257637024\n",
            "3100 0.34900110960006714\n",
            "3200 0.3488978147506714\n",
            "3300 0.3488028347492218\n",
            "3400 0.3487151265144348\n",
            "3500 0.34863391518592834\n",
            "3600 0.34855854511260986\n",
            "3700 0.34848839044570923\n",
            "3800 0.34842291474342346\n",
            "3900 0.348361611366272\n",
            "4000 0.34830427169799805\n",
            "4100 0.3482503890991211\n",
            "4200 0.348199725151062\n",
            "4300 0.34815192222595215\n",
            "4400 0.34810686111450195\n",
            "4500 0.3480641841888428\n",
            "4600 0.34802383184432983\n",
            "4700 0.34798550605773926\n",
            "4800 0.34794920682907104\n",
            "4900 0.3479146361351013\n",
            "5000 0.3478817343711853\n",
            "5100 0.34785041213035583\n",
            "5200 0.347820520401001\n",
            "5300 0.3477919101715088\n",
            "5400 0.34776461124420166\n",
            "5500 0.34773847460746765\n",
            "5600 0.347713440656662\n",
            "5700 0.3476894795894623\n",
            "5800 0.3476664423942566\n",
            "5900 0.3476443290710449\n",
            "6000 0.3476230502128601\n",
            "6100 0.34760263562202454\n",
            "6200 0.34758299589157104\n",
            "6300 0.34756407141685486\n",
            "6400 0.3475457429885864\n",
            "6500 0.3475281596183777\n",
            "6600 0.3475111424922943\n",
            "6700 0.34749478101730347\n",
            "6800 0.3474789559841156\n",
            "6900 0.34746360778808594\n",
            "7000 0.34744876623153687\n",
            "7100 0.3474344313144684\n",
            "7200 0.34742051362991333\n",
            "7300 0.3474070727825165\n",
            "7400 0.3473939895629883\n",
            "7500 0.3473813533782959\n",
            "7600 0.34736913442611694\n",
            "7700 0.34735721349716187\n",
            "7800 0.34734565019607544\n",
            "7900 0.34733444452285767\n",
            "8000 0.347323477268219\n",
            "8100 0.34731292724609375\n",
            "8200 0.3473026156425476\n",
            "8300 0.3472925126552582\n",
            "8400 0.347282737493515\n",
            "8500 0.34727323055267334\n",
            "8600 0.34726399183273315\n",
            "8700 0.3472549617290497\n",
            "8800 0.3472461998462677\n",
            "8900 0.34723758697509766\n",
            "9000 0.3472291827201843\n",
            "9100 0.3472210168838501\n",
            "9200 0.34721308946609497\n",
            "9300 0.34720534086227417\n",
            "9400 0.3471977114677429\n",
            "9500 0.347190260887146\n",
            "9600 0.3471830487251282\n",
            "9700 0.3471760153770447\n",
            "9800 0.3471691310405731\n",
            "9900 0.3471623659133911\n",
            "10000 0.34715571999549866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
        "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "# model\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\n",
        "\n",
        "for step in range(10001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item())\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "with torch.no_grad():\n",
        "    predicted = (model(X) > 0.5).float()\n",
        "    accuracy = (predicted == Y).float().mean()\n",
        "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
      ],
      "metadata": {
        "id": "OE_J-vVN7oeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4907ad1d-c7a2-4dbc-da65-68cbbcfa1cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.7434072494506836\n",
            "100 0.6931650638580322\n",
            "200 0.6931577920913696\n",
            "300 0.6931517124176025\n",
            "400 0.6931463479995728\n",
            "500 0.6931411027908325\n",
            "600 0.693135678768158\n",
            "700 0.6931295394897461\n",
            "800 0.693122148513794\n",
            "900 0.6931126713752747\n",
            "1000 0.6930999755859375\n",
            "1100 0.693082332611084\n",
            "1200 0.6930568814277649\n",
            "1300 0.6930190920829773\n",
            "1400 0.6929606199264526\n",
            "1500 0.6928659677505493\n",
            "1600 0.6927032470703125\n",
            "1700 0.6923960447311401\n",
            "1800 0.6917300820350647\n",
            "1900 0.6899654865264893\n",
            "2000 0.6838319301605225\n",
            "2100 0.6561682224273682\n",
            "2200 0.4311133623123169\n",
            "2300 0.13489647209644318\n",
            "2400 0.06630530953407288\n",
            "2500 0.042168572545051575\n",
            "2600 0.03045409731566906\n",
            "2700 0.02366606518626213\n",
            "2800 0.019277825951576233\n",
            "2900 0.016224082559347153\n",
            "3000 0.013983854092657566\n",
            "3100 0.012273991480469704\n",
            "3200 0.010928163304924965\n",
            "3300 0.009842487052083015\n",
            "3400 0.008949032984673977\n",
            "3500 0.008201336488127708\n",
            "3600 0.007566767744719982\n",
            "3700 0.007021686062216759\n",
            "3800 0.006548595614731312\n",
            "3900 0.006134253926575184\n",
            "4000 0.005768374539911747\n",
            "4100 0.0054430365562438965\n",
            "4200 0.005151890218257904\n",
            "4300 0.0048899175599217415\n",
            "4400 0.004652872681617737\n",
            "4500 0.004437457304447889\n",
            "4600 0.004240859299898148\n",
            "4700 0.00406070239841938\n",
            "4800 0.0038950315210968256\n",
            "4900 0.003742194501683116\n",
            "5000 0.003600734518840909\n",
            "5100 0.0034694799687713385\n",
            "5200 0.0033473046496510506\n",
            "5300 0.0032333978451788425\n",
            "5400 0.0031268750317394733\n",
            "5500 0.0030270610004663467\n",
            "5600 0.0029333699494600296\n",
            "5700 0.0028452035039663315\n",
            "5800 0.002762140706181526\n",
            "5900 0.0026837773621082306\n",
            "6000 0.0026096487417817116\n",
            "6100 0.0025394847616553307\n",
            "6200 0.0024729417636990547\n",
            "6300 0.0024097643326967955\n",
            "6400 0.0023496984504163265\n",
            "6500 0.0022925634402781725\n",
            "6600 0.002238075714558363\n",
            "6700 0.002186085097491741\n",
            "6800 0.0021364721469581127\n",
            "6900 0.002089011948555708\n",
            "7000 0.0020436146296560764\n",
            "7100 0.0020001311786472797\n",
            "7200 0.001958396751433611\n",
            "7300 0.0019184107659384608\n",
            "7400 0.0018799942918121815\n",
            "7500 0.0018430722411721945\n",
            "7600 0.0018075400730594993\n",
            "7700 0.0017733527347445488\n",
            "7800 0.0017404207028448582\n",
            "7900 0.0017087138257920742\n",
            "8000 0.001678097527474165\n",
            "8100 0.0016485570231452584\n",
            "8200 0.001620002556592226\n",
            "8300 0.0015924491453915834\n",
            "8400 0.0015657917829230428\n",
            "8500 0.0015400308184325695\n",
            "8600 0.0015150615945458412\n",
            "8700 0.001490913680754602\n",
            "8800 0.0014674977865070105\n",
            "8900 0.001444813678972423\n",
            "9000 0.0014228166546672583\n",
            "9100 0.0014014765620231628\n",
            "9200 0.0013806892093271017\n",
            "9300 0.0013605887070298195\n",
            "9400 0.0013410557294264436\n",
            "9500 0.001322030322626233\n",
            "9600 0.001303557539358735\n",
            "9700 0.001285637030377984\n",
            "9800 0.0012681199004873633\n",
            "9900 0.0012511102249845862\n",
            "10000 0.0012345188297331333\n",
            "\n",
            "Hypothesis:  [[0.00106378]\n",
            " [0.9988938 ]\n",
            " [0.9988939 ]\n",
            " [0.00165883]] \n",
            "Correct:  [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code: xor-nn-wide-deep"
      ],
      "metadata": {
        "id": "JjCSMhyysIlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device='cuda')\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device='cuda')\n",
        "\n",
        "## nn Layers\n",
        "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
        "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device='cuda')\n",
        "\n",
        "# define cost/Loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device='cuda')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "for step in range(10001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/Loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQh5hlppr9oF",
        "outputId": "ab3d3238-5807-4c30-feb0-9faf8ef05b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.6978083848953247\n",
            "100 0.6931105256080627\n",
            "200 0.6931018829345703\n",
            "300 0.6930921673774719\n",
            "400 0.6930808424949646\n",
            "500 0.6930675506591797\n",
            "600 0.6930519342422485\n",
            "700 0.69303297996521\n",
            "800 0.6930098533630371\n",
            "900 0.6929808855056763\n",
            "1000 0.6929439902305603\n",
            "1100 0.692895770072937\n",
            "1200 0.6928311586380005\n",
            "1300 0.6927415132522583\n",
            "1400 0.6926121711730957\n",
            "1500 0.6924156546592712\n",
            "1600 0.6920979022979736\n",
            "1700 0.6915386915206909\n",
            "1800 0.6904336214065552\n",
            "1900 0.6878381371498108\n",
            "2000 0.6797102689743042\n",
            "2100 0.6408318281173706\n",
            "2200 0.5488757491111755\n",
            "2300 0.5083184242248535\n",
            "2400 0.487520694732666\n",
            "2500 0.31329506635665894\n",
            "2600 0.020284105092287064\n",
            "2700 0.0077512226998806\n",
            "2800 0.004524867050349712\n",
            "2900 0.003117209766060114\n",
            "3000 0.0023459556978195906\n",
            "3100 0.0018651896389201283\n",
            "3200 0.0015393730718642473\n",
            "3300 0.0013052920112386346\n",
            "3400 0.0011296711163595319\n",
            "3500 0.0009934453992173076\n",
            "3600 0.0008848900906741619\n",
            "3700 0.000796599080786109\n",
            "3800 0.0007234817021526396\n",
            "3900 0.000661970698274672\n",
            "4000 0.0006095886928960681\n",
            "4100 0.0005644556949846447\n",
            "4200 0.0005252138944342732\n",
            "4300 0.0004908637492917478\n",
            "4400 0.0004604355781339109\n",
            "4500 0.00043342227581888437\n",
            "4600 0.00040921231266111135\n",
            "4700 0.00038746261270716786\n",
            "4800 0.0003677854547277093\n",
            "4900 0.00034988258266821504\n",
            "5000 0.0003336047811899334\n",
            "5100 0.00031866884091868997\n",
            "5200 0.0003049553488381207\n",
            "5300 0.00029233022360131145\n",
            "5400 0.0002806740812957287\n",
            "5500 0.0002697931486181915\n",
            "5600 0.0002597321290522814\n",
            "5700 0.00025038665626198053\n",
            "5800 0.00024160764587577432\n",
            "5900 0.00023342490021605045\n",
            "6000 0.00022576389892492443\n",
            "6100 0.00021852027566637844\n",
            "6200 0.00021172384731471539\n",
            "6300 0.00020534478244371712\n",
            "6400 0.0001992638426600024\n",
            "6500 0.00019357044948264956\n",
            "6600 0.00018814536451827735\n",
            "6700 0.00018304819241166115\n",
            "6800 0.00017815970932133496\n",
            "6900 0.00017353951989207417\n",
            "7000 0.00016912799037527293\n",
            "7100 0.0001649251498747617\n",
            "7200 0.00016093096928671002\n",
            "7300 0.00015710074512753636\n",
            "7400 0.00015346426516771317\n",
            "7500 0.00014994705270510167\n",
            "7600 0.0001466086832806468\n",
            "7700 0.00014338955224957317\n",
            "7800 0.00014033436309546232\n",
            "7900 0.0001373835257254541\n",
            "8000 0.0001345369964838028\n",
            "8100 0.00013180970563553274\n",
            "8200 0.000129216568893753\n",
            "8300 0.00012665321992244571\n",
            "8400 0.0001242389262188226\n",
            "8500 0.00012188424443593249\n",
            "8600 0.00011961898417212069\n",
            "8700 0.00011745805386453867\n",
            "8800 0.00011535674275364727\n",
            "8900 0.00011331503628753126\n",
            "9000 0.00011131805513286963\n",
            "9100 0.00010942538938252255\n",
            "9200 0.0001075923428288661\n",
            "9300 0.0001057742047123611\n",
            "9400 0.0001040752831613645\n",
            "9500 0.00010239127732347697\n",
            "9600 0.00010073707380797714\n",
            "9700 9.918719297274947e-05\n",
            "9800 9.76671144599095e-05\n",
            "9900 9.61619516601786e-05\n",
            "10000 9.473130194237456e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab-09-1 ReLU"
      ],
      "metadata": {
        "id": "1bdArZaHs4U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습목표\n",
        "\n",
        "ReLU 활성화 함수에 대해 알아본다."
      ],
      "metadata": {
        "id": "Oxzv5WO_s7gB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 핵심키워드\n",
        "\n",
        "ReLU  \n",
        "Sigmoid  \n",
        "Optimizer"
      ],
      "metadata": {
        "id": "cdPvwu9bs9yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU\n",
        "\n",
        "$f(x) = max(0, x)$\n",
        "\n",
        "```python\n",
        "x = torch.nn.sigmoid(x)\n",
        "x = torch.nn.relu(x)\n",
        "```"
      ],
      "metadata": {
        "id": "JpB5wUDKtj0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "y = np.maximum(0, x)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(x, y)\n",
        "plt.legend(['Relu'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "Dmt2orHcsUZa",
        "outputId": "2a4cdf0e-2c75-4c98-9a01-c07870d1e23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAEvCAYAAACdahL0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVeL+8eeQSugl1AAJvUMKCNh77xWl7CpEQNf+dXXV1V3dYlldC7ICukuoIva29rb2JAQIvUNCSyihJqSc3x8Z/SEmEDKTOVM+79crr0xmbmaemzszz9w7Z06MtVYAAMC/6rkOAABAOKKAAQBwgAIGAMABChgAAAcoYAAAHKCAAQBwINKfN9ayZUubmJjoz5sEAMCZrKysQmttfFWX+bWAExMTlZmZ6c+bBADAGWPM+uou4xA0AAAOUMAAADhAAQMA4IBf3wOuSmlpqfLy8lRcXOw6Sp2JjY1VQkKCoqKiXEcBAAQI5wWcl5enRo0aKTExUcYY13F8zlqr7du3Ky8vT0lJSa7jAAAChPND0MXFxWrRokVIlq8kGWPUokWLkN7DBwAcO+cFLClky/cnob5+AIBjd9QCNsa8ZIzZZozJPeS85saYj4wxKz3fm9VtzLoVERGhgQMHqm/fvrrwwgu1a9euIy7/0EMP6YknnvBTOgBAKKrJHvB/JJ1z2Hn3SPrEWttN0ieen4NW/fr1lZOTo9zcXDVv3lwTJ050HQkAEOKOWsDW2i8l7Tjs7IslTfOcnibpEh/ncmbo0KHKz8+XJK1evVrnnHOOUlNTdeKJJ2rZsmW/Wv6UU075eXavwsJCMdUmAASnDxdvUW5+kd9ur7bvAbe21m72nN4iqXV1Cxpj0o0xmcaYzIKCglrenH+Ul5frk08+0UUXXSRJSk9P17PPPqusrCw98cQTmjBhguOEAIC6kJtfpFvmzNdjHyz32216/TEka601xtgjXD5Z0mRJSktLq3Y5SfrT24u1ZNNubyP9Qu92jfXghX2OuMyBAwc0cOBA5efnq1evXjrzzDO1d+9effPNN7ryyit/Xq6kpMSn2QAA7m3bU6yxGZlq0SBGT141wG+3W9s94K3GmLaS5Pm+zXeR/O+n94DXr18va60mTpyoiooKNW3aVDk5OT9/LV269Fe/GxkZqYqKCknio0YAEGRKyso1bnqWdu0v1eRRqWrZMMZvt13bPeC3JI2W9HfP9zd9EeZoe6p1LS4uTs8884wuueQSTZgwQUlJSXrllVd05ZVXylqrhQsXasCAX746SkxMVFZWlgYPHqx58+Y5Sg4AOFbWWt33eq6yN+zSpOtS1KddE7/efk0+hjRb0reSehhj8owxN6iyeM80xqyUdIbn55CQnJys/v37a/bs2Zo5c6ZefPFFDRgwQH369NGbb/76dcZdd92lSZMmKTk5WYWFhQ4SAwBq48X/rdW8rDzddkY3nduvrd9v31h7xLdlfSotLc0e/v+Aly5dql69evktgyvhsp4AEAy+WFGg3/77B53dp40mXpuievXqZsIkY0yWtTatqssCYiYsAAD8ZU3BXt08K1s92jTWP64aUGflezQUMAAgbBQdKNWYjExFR9TTlFGpiot29z+JnP83JAAA/KG8wuqW2fO1ccd+zRo7RAnN4pzmCYg9YH++D+1CqK8fAASDv7+/VF+sKNDDF/fVoMTmruO4L+DY2Fht3749ZEvqp/8HHBsb6zoKAISteVl5mvLVWv1mWKKuGdzRdRxJAXAIOiEhQXl5eQr0aSq9ERsbq4SEBNcxACAsZW/YqT+8tkjHd22h+88PnE+jOC/gqKgoJSUluY4BAAhBm4sO6MbpWWrbNFYTr01RZITzA78/c17AAADUhQMHy5WekaUDB8s1a8xxahoX7TrSL1DAAICQY63V3a8uVO6mIk0dlaZurRu5jvQrgbMvDgCAjzz/+Wq9vWCT7j67p07vVe1/zHWKAgYAhJSPlmzVEx8u1yUD22ncyZ1dx6kWBQwACBnLt+zRbXPmq3/7Jvr75f1ljJtpJmuCAgYAhISd+w5qTMaPahATqRdGpik2KsJ1pCNiEBYAIOiVlldowsxsbd1dopfTh6hNk8Cf/Ig9YABA0Hv4nSX6ds12PXp5PyV3bOY6To1QwACAoDbz+/XK+Ha9bjy5sy5NDp5ZBylgAEDQ+m7Ndj345mKd2iNed5/d03WcY0IBAwCC0sYd+zV+RpY6tYjT08OTFVEvcEc8V4UCBgAEnX0lZRqbkanyCqupowepcWyU60jHjFHQAICgUlFhdcfcHK3ctlf/+e0gJbVs4DpSrbAHDAAIKv/8eIU+WLxV95/fSyd2i3cdp9YoYABA0Hhn4SY98+kqXZ3WQb8Zlug6jlcoYABAUMjNL9JdryxQWqdm+vMlfQJ6msmaoIABAAGvYE+J0jMy1TwuWpNGpComMrCnmawJBmEBAAJaSVm5xs3I0s79pZo3fqjiG8W4juQTFDAAIGBZa3X/67nKWr9TE69NUZ92TVxH8hkOQQMAAta/v16nV7LydMvp3XR+/7au4/gUBQwACEhfrijQI+8u0dl9Wuu207u5juNzFDAAIOCsLdynm2dlq3vrRnryqoGqF2TTTNYEBQwACCi7i0s1ZtqPioyopymj0tQgJjSHK1HAAICAUV5hdcvs+Vq/fb8mXZeiDs3jXEeqM6H5sgIAEJQe++8yfb68QH+9tJ+O69zCdZw6xR4wACAgvJadpxe+XKNRQzvp2uM6uo5T5yhgAIBz8zfs1D2vLdKwLi30wAW9XcfxCwoYAODUlqJipU/PUpvGsZp4bYqiIsKjmngPGADgTHFpudKnZ2p/SZlmjjlOzRpEu47kNxQwAMAJa61+/+pCLcov0pSRaereupHrSH4VHvv5AICAM+mL1XozZ5PuOquHzujd2nUcv6OAAQB+9/GSrXr8g+W6aEA7TTili+s4TlDAAAC/WrF1j26dM1992zXRY1f0lzGhN81kTXhVwMaY240xi40xucaY2caYWF8FAwCEnp37DmrMtEzFxURq8qhUxUZFuI7kTK0L2BjTXtItktKstX0lRUi6xlfBAAChpbS8QjfNytaW3cV6YWSq2jap7zqSU94ego6UVN8YEykpTtIm7yMBAELRI+8s0Tert+tvl/ZTSsdmruM4V+sCttbmS3pC0gZJmyUVWWs/9FUwAEDomPX9Bk37dr3ST+qsy1MTXMcJCN4cgm4m6WJJSZLaSWpgjBlRxXLpxphMY0xmQUFB7ZMCAILS92u2649v5urk7vH6/Tk9XccJGN4cgj5D0lprbYG1tlTSa5KGHb6QtXaytTbNWpsWHx/vxc0BAIJN3s79Gj8zWx1bxOmZ4cmKqBeeI56r4k0Bb5A0xBgTZyrHkJ8uaalvYgEAgt2+kjKNmZapsvIKTR2Vpib1o1xHCijevAf8vaR5krIlLfJc12Qf5QIABLGKCqs75uZoxdY9eu7aFHWOb+g6UsDxai5oa+2Dkh70URYAQIh4+pOV+mDxVj1wQW+d1J23H6vCTFgAAJ96b9FmPf3JSl2ZmqDrj090HSdgUcAAAJ9ZvKlId85doNROzfTIpX3DdprJmqCAAQA+Ubi3ROkZWWoaF6V/jUhVTGT4TjNZE/w/YACA1w6WVWjc9Cxt31eieeOGKb5RjOtIAY8CBgB4xVqrB97IVeb6nXru2mT1bd/EdaSgwCFoAIBXpn2zTi9nbtTvTuuqC/q3cx0naFDAAIBa+9/KQj387lKd1bu1bj+ju+s4QYUCBgDUytrCfbppVra6tWqop64eqHpMM3lMKGAAwDHbXVyqsRmZqmekKaPS1CCGIUXHir8YAOCYlFdY3TYnR+sK92n6DcepQ/M415GCEgUMADgmj32wTJ8u26ZHLumroV1auI4TtDgEDQCosdfn5+mFL9Zo5JBOGjGkk+s4QY0CBgDUSM7GXfr9q4s0pHNz/fHC3q7jBD0KGABwVFt3Fys9I1OtG8fo+etSFRVBfXiLvyAA4IiKS8uVnpGpfSVlmjpqkJo3iHYdKSQwCAsAUC1rre55daEW5hfphRGp6tGmketIIYM9YABAtV74co3eyNmku87qobP6tHEdJ6RQwACAKn26bKse/e8yXdC/rSac0sV1nJBDAQMAfmXVtj26ZXaO+rRrrMevGCBjmGbS1yhgAMAv7Np/UDdMy1RsVIQmj0xT/egI15FCEgUMAPhZWXmFbpqVrc27ivXCyFS1a1rfdaSQxShoAMDPHnl3qb5etV1PXDlAqZ2auY4T0tgDBgBIkub8sEH/+WadxpyQpCtSE1zHCXkUMABAP67boQfezNVJ3eN1z7k9XccJCxQwAIS5vJ37NW56ljo0i9Ozw5MVyTSTfsFfGQDC2P6DZRqbkaWD5RWaMjpNTepHuY4UNihgAAhTFRVWd85doOVbduu5a1PUJb6h60hhhQIGgDD17Ker9H7uFv3hvF46uXu86zhhhwIGgDD0/qLNeurjFbo8JUE3nJDkOk5YooABIMws2bRbd8xdoOSOTfWXS/syzaQjFDAAhJHCvSUam5GppnFRemFkqmKjmGbSFWbCAoAwcbCsQhNmZKtwb4nmjRumVo1iXUcKaxQwAIQBa60efCtXP6zboWeGJ6tfQhPXkcIeh6ABIAxkfLtes3/YqJtO7aKLBrRzHQeigAEg5H29qlB/fmeJzuzdWnee2cN1HHhQwAAQwtZv36cJM7PVJb6Bnrp6oOrVY8RzoKCAASBE7Sku1ZhpmTJGmjpqkBrGMOwnkLA1ACAElVdY3TYnR2sK92n6DYPVsUWc60g4DHvAABCCnvhwuT5Ztk0PXdhbw7q0dB0HVaCAASDEvJmTr0mfr9Z1x3XUyKGJruOgGl4VsDGmqTFmnjFmmTFmqTFmqK+CAQCO3YKNu3T3vIU6Lqm5Hrywj+s4OAJv3wN+WtJ/rbVXGGOiJfEmAwA4snV3sdKnZyq+UYyevy5F0ZEc5AxktS5gY0wTSSdJ+o0kWWsPSjrom1gAgGNRXFqu9OlZ2lNcplfHD1OLhjGuI+EovHl5lCSpQNK/jTHzjTFTjTENfJQLAFBD1lr94bVFWrBxl566eqB6tW3sOhJqwJsCjpSUImmStTZZ0j5J9xy+kDEm3RiTaYzJLCgo8OLmAABVmfLVGr02P193ntldZ/dp4zoOasibAs6TlGet/d7z8zxVFvIvWGsnW2vTrLVp8fHxXtwcAOBwny3bpr+9v0zn92urm0/r6joOjkGtC9hau0XSRmPMTxOLni5piU9SAQCOatW2Pbpl9nz1bttYj1/ZX8YwzWQw8XYU9O8kzfSMgF4j6bfeRwIAHE3R/sppJmOi6mnyqDTFRTOxYbDxaotZa3MkpfkoCwCgBsrKK3Tz7Gzl7zqgOelD1L5pfdeRUAu8ZAKAIPPX95bpq5WFeuzy/krt1Nx1HNQSn9IGgCAy98eNeunrtbr++CRdNaiD6zjwAgUMAEEic90O3ffGIp3YraX+cF5P13HgJQoYAIJA/q4DGjcjSwnN4vTc8BRFRvD0Hex4DxgAAtz+g2UaOy1TJaUVmpOepiZxUa4jwQcoYAAIYNZa/d8rC7V0y2699JtB6tqqoetI8BGOYQBAAHv201V6d9Fm3XtuT53ao5XrOPAhChgAAtR/c7foyY9W6LLk9hp7YmfXceBjFDAABKClm3frjrk5Gtihqf56WT+mmQxBFDAABJjte0s0ZlqmGsVGavLIVMVGRbiOhDrAICwACCAHyyo0fma2CveWaO6NQ9WqcazrSKgjFDAABAhrrR58a7F+WLtDT18zUAM6NHUdCXWIQ9AAECBmfLdes3/YoAmndNHFA9u7joM6RgEDQAD4ZlWhHnp7ic7o1Up3ndXj6L+AoEcBA4Bj67fv04RZ2ercsoGeunqg6tVjxHM4oIABwKE9xaUaMy1T1kpTR6epUSzTTIYLBmEBgCMVFVa3v5yjNYX7NP36werUooHrSPAj9oABwJF/fLRcHy/dpgcv7K1hXVu6jgM/o4ABwIE3c/I18bPVGj64o0YO6eQ6DhyggAHAzxbm7dLd8xZqcGJz/emiPkwzGaYoYADwo227i5WekaWWDWM0aUSKoiN5Gg5XDMICAD8pLi1X+vQs7S4u1avjh6lFwxjXkeAQBQwAfmCt1X2v5ypn4y79a0SKerVt7DoSHOPYBwD4wdSv1urV7DzdfkZ3ndO3res4CAAUMADUsc+Wb9Pf3l+q8/q10e9O6+o6DgIEBQwAdWjVtr26ZdZ89WzTWE9cOYBpJvEzChgA6kjR/lKlZ2QqOrKepoxOU1w0w27w/3FvAIA6UFZeoZtnZ2vjzv2aNXaI2jet7zoSAgwFDAB14G/vL9NXKwv16OX9NCixues4CEAcggYAH5ubuVEv/m+tfjMsUVcP6ug6DgIUBQwAPpS1fofufz1XJ3RtqfvP7+U6DgIYBQwAPrJp1wHdOD1b7ZrG6rlrkxUZwVMsqsd7wADgAwcOlmtsRqaKS8s1J/04NY2Ldh0JAY4CBgAvWWt117wFWrJ5t14cnaaurRq5joQgwPERAPDSxM9W6d2Fm/X7c3rqtJ6tXcdBkKCAAcALHy7eoic+XKFLk9vrxpM6u46DIEIBA0AtLduyW7e9nKMBHZrqb5f1kzFMM4mao4ABoBZ27DuoMdMy1TAmUpNHpio2KsJ1JAQZBmEBwDEqLa/Q+BlZ2ranRHNvHKrWjWNdR0IQYg8YAI7Rn95erO/X7tBjl/fXwA5NXcdBkKKAAeAYTP9uvWZ8t0HjTu6iS5Lbu46DIOZ1ARtjIowx840x7/giEAAEqm9WF+pPby3WaT1b6f/O7uE6DoKcL/aAb5W01AfXAwABa8P2/bppZrYSWzbQ09cMVEQ9RjzDO14VsDEmQdL5kqb6Jg4ABJ69JWUam5GpCitNHZWmRrFRriMhBHi7B/xPSXdLqvBBFgAIOBUVVre/nKNVBXv1/HUpSmzZwHUkhIhaF7Ax5gJJ26y1WUdZLt0Yk2mMySwoKKjtzQGAE09+tEIfLdmqB87vpeO7tnQdByHEmz3g4yVdZIxZJ2mOpNOMMTMOX8haO9lam2atTYuPj/fi5gDAv95esEnPfbZK1wzqoNHDEl3HQYipdQFba++11iZYaxMlXSPpU2vtCJ8lAwCHFuUV6f/mLdCgxGb688V9mWYSPsfngAHgMNv2FCt9eqZaNIjRpBGpio7kqRK+55OpKK21n0v63BfXBQAulZSVa9z0LO3aX6p544eqZcMY15EQopgLGgA8rLW67/VcZW/YpUnXpahPuyauIyGEcVwFADxe/N9azcvK062nd9O5/dq6joMQRwEDgKQvVhTor+8t1bl92+jW07u5joMwQAEDCHurC/bq5lnZ6tGmsf5x1QDVY5pJ+AEFDCCsFR0o1dhpmYqOqKcpo1IVF83QGPgH9zQAYau8wup3s+drw479mjV2iBKaxbmOhDBCAQMIW39/f6m+XFGgv13WT4OTmruOgzDDIWgAYWleVp6mfLVWo4d20vDBHV3HQRiigAGEnaz1O/WH1xbp+K4t9MAFvV3HQZiigAGElc1FB3Tj9Cy1bRqr54anKDKCp0G4wXvAAMLGgYPlSs/IUnFpuWaNPU7NGkS7joQwRgEDCAvWWt396kLlbirS1FFp6t66ketICHMcewEQFp7/fLXeXrBJd5/dU6f3au06DkABAwh9Hy7eosc/WK6LB7bTuJM7u44DSKKAAYS45Vv26PaXc9Q/oYkevby/jGGaSQQGChhAyNq576DGZPyouJhITR6ZptioCNeRgJ8xCAtASCotr9CEmdnaurtEL6cPUZsmsa4jAb/AHjCAkPTnt5fo2zXb9ffL+im5YzPXcYBfoYABhJwZ363X9O/W68aTOuuylATXcYAqUcAAQsp3a7brobcW65Qe8br7nJ6u4wDVooABhIyNO/Zr/IwsdWoRp2eGJyuiHiOeEbgoYAAhYW9JmcZmZKq8wmrq6EFqHBvlOhJwRIyCBhD0Kiqs7ng5Ryu27tG06wcrqWUD15GAo2IPGEDQ++fHK/Thkq26//zeOrFbvOs4QI1QwACC2jsLN+mZT1fpqrQE/fb4RNdxgBqjgAEErdz8It31ygKldmqmhy/pyzSTCCoUMICgVLCnRGMzMtU8Llr/GpGqmEimmURwYRAWgKBTUlaucTOytHP/Qc0bN0zxjWJcRwKOGQUMIKhYa3X/67nKWr9TE69NUd/2TVxHAmqFQ9AAgsq/v16nV7LydMtpXXV+/7au4wC1RgEDCBpfrijQI+8u0dl9Wuu2M7q7jgN4hQIGEBTWFOzVzbOy1b11Iz151UDVY5pJBDkKGEDA211cqjEZmYqMqKcpo9LUIIbhKwh+FDCAgFZeYXXL7PnasH2/nr8uRR2ax7mOBPgELyMBBLRH/7tMny8v0F8u7ashnVu4jgP4DHvAAALWq1l5mvzlGo0a2knXHdfJdRzApyhgAAEpe8NO3fvaIg3t3EIPXNDbdRzA5yhgAAFnS1GxbpyepdZNYvT8dSmKiuCpCqGHezWAgFJcWq706ZnaX1KmF0cPUrMG0a4jAXWCQVgAAoa1VnfPW6hF+UWaMjJN3Vs3ch0JqDO13gM2xnQwxnxmjFlijFlsjLnVl8EAhJ9JX6zWWws26a6zeuiM3q1dxwHqlDd7wGWS7rTWZhtjGknKMsZ8ZK1d4qNsAMLIx0u26vEPluvCAe004ZQuruMAda7We8DW2s3W2mzP6T2Slkpq76tgAMLHiq17dOuc+erbrokeu7y/jGGaSYQ+nwzCMsYkSkqW9L0vrg9A+Ni576DGTMtUXEykJo9KVf3oCNeRAL/wuoCNMQ0lvSrpNmvt7iouTzfGZBpjMgsKCry9OQAhpLS8QhNmZmtLUbFeGJmqtk3qu44E+I1XBWyMiVJl+c601r5W1TLW2snW2jRrbVp8fLw3NwcgxDzyzhJ9u2a7/npZP6V0bOY6DuBX3oyCNpJelLTUWvuk7yIBCAezvt+gad+u19gTk3RFaoLrOIDfebMHfLykkZJOM8bkeL7O81EuACHs+zXb9cc3c3Vy93jdc24v13EAJ2r9MSRr7f8kMVQRwDHZuGO/xs/MVscWcXpmeLIi6vE0gvDEVJQA/GZfSZnGZmSqtLxCU0elqUn9KNeRAGcoYAB+UVFhdcfcHK3YukfPXZuizvENXUcCnKKAAfjF05+s1AeLt+oP5/XSyd35RARAAQOoc+8u3KynP1mpK1ITdMMJSa7jAAGBAgZQp3Lzi3TnKzlK6dhUf7m0L9NMAh4UMIA6U7CnROkZmWoWF61/jUxVTCTTTAI/4f8BA6gTB8sqNH5GlnbsP6h544apVaNY15GAgEIBA/A5a60eeCNXmet36tnhyerbvonrSEDA4RA0AJ/7zzfr9HLmRt18alddOKCd6zhAQKKAAfjUVysL9PA7S3Rm79a648zuruMAAYsCBuAzawv36eZZ89WtVSM9dfVA1WOaSaBaFDAAn9hdXKox035UPSNNHZ2mhjEMMQGOhEcIAK+VV1jdOnu+1m/fr+k3HKcOzeNcRwICHgUMwGuPfbBMny0v0COX9NXQLi1cxwGCAoegAXjl9fl5euGLNRoxpKNGDOnkOg4QNChgALWWs3GXfv/qIg3p3FwPXtjHdRwgqFDAAGplS1Gx0jMy1apRjJ6/LlVRETydAMeCRwyAY1ZcWq4bp2dqX0mZpo5OU/MG0a4jAUGHQVgAjom1Vve8ulAL8oo0eWSqerZp7DoSEJTYAwZwTF74co3eyNmku87qrrP6tHEdBwhaFDCAGvtk6VY9+t9luqB/W910alfXcYCgRgEDqJGVW/fo1jk56tOusR6/YoCMYZpJwBsUMICj2rX/oMZkZCo2KkKTR6apfnSE60hA0KOAARxRWXmFbpqVrc27ivXCyFS1a1rfdSQgJDAKGsARPfLuUn29arseu6K/Ujs1cx0HCBnsAQOo1pwfNug/36zTDSck6aq0Dq7jACGFAgZQpR/W7tADb+bqpO7xuvfcnq7jACGHAgbwK3k792v8jCx1aBanZ4cnK5JpJgGf41EF4Bf2HyzT2IwsHSyv0JTRaWpSP8p1JCAkUcAAflZRYXXn3AVavmW3nhmerC7xDV1HAkIWBQzgZ898ulLv527Rvef20qk9WrmOA4Q0ChiAJOn9RZv1z49X6vKUBI05Mcl1HCDkUcAAtGTTbt0xd4GSOzbVXy7tyzSTgB9QwECYK9xborEZmWpSP0ovjEhVbBTTTAL+wExYQBg7WFah8TOyVLi3RK+MG6pWjWNdRwLCBgUMhClrrf74Zq5+XLdTT18zUP0TmrqOBIQVDkEDYSrj2/Wa8+NG3XRqF108sL3rOEDYoYCBMPT1qkL9+Z0lOqNXK915Zg/XcYCwRAEDYWZd4T5NmJmtLvEN9NTVA1WvHiOeARcoYCCM7Cku1ZiMTBkjTR01SI1imWYScIVBWECYKK+wunVOjtYW7tP0GwarY4s415GAsObVHrAx5hxjzHJjzCpjzD2+CgXA9574cLk+XbZND13YW8O6tHQdBwh7tS5gY0yEpImSzpXUW9JwY0xvXwUD4DtvzM/XpM9X69rjOmrEkE6u4wCQd4egB0taZa1dI0nGmDmSLpa0xBfBjiY3v0ibi4r9cVNAUNu+t0R/fGuxBic110MX9mGaSSBAeFPA7SVtPOTnPEnHHb6QMSZdUrokdezY0Yub+6WX/rdWr83P99n1AaEsqWUDTbouRdGRjLsEAkWdD8Ky1k6WNFmS0tLSrK+u9/Yzu+v6E/iPLUBNdG3VkDmegQDjTQHnS+pwyM8JnvP8okPzuF/cOAAAwcSb41E/SupmjEkyxkRLukbSW76JBQBAaKv1HrC1tswYc7OkDyRFSHrJWrvYZ8kAAAhhXr0HbK19T9J7PsoCAEDYYEgkAAAOUMAAADhAAQMA4AAFDACAAxQwAAAOUMAAADhAAQMA4ICx1mfTMx/9xowpkLTeh1fZUlKhD6/PJdYl8ITKekisSyAKlfWQWJcj6WStja/qAr8WsK8ZYzKttWmuc/gC6xJ4QmU9JNYlEIXKekisS21xCBoAAAcoYAAAHAj2AnjFdEMAAAY2SURBVJ7sOoAPsS6BJ1TWQ2JdAlGorIfEutRKUL8HDABAsAr2PWAAAIJSwBewMeZKY8xiY0yFMSbtsMvuNcasMsYsN8acXc3vJxljvvcs97IxJto/yY/MkyXH87XOGJNTzXLrjDGLPMtl+jtnTRhjHjLG5B+yPudVs9w5nm21yhhzj79zHo0x5nFjzDJjzEJjzOvGmKbVLBew2+Rof2NjTIznvrfK87hI9H/KozPGdDDGfGaMWeJ5/N9axTKnGGOKDrnf/dFF1qM52v3FVHrGs00WGmNSXOQ8GmNMj0P+1jnGmN3GmNsOWyZgt4kx5iVjzDZjTO4h5zU3xnxkjFnp+d6smt8d7VlmpTFmtM9CWWsD+ktSL0k9JH0uKe2Q83tLWiApRlKSpNWSIqr4/bmSrvGc/pek8a7XqYqM/5D0x2ouWyeppeuMR8n/kKS7jrJMhGcbdZYU7dl2vV1nPyzjWZIiPacflfRoMG2TmvyNJU2Q9C/P6Wskvew6dzXr0lZSiud0I0krqliXUyS94zprDdbliPcXSedJel+SkTRE0veuM9dgnSIkbVHlZ1yDYptIOklSiqTcQ857TNI9ntP3VPWYl9Rc0hrP92ae0818kSng94CttUuttcuruOhiSXOstSXW2rWSVkkafOgCxhgj6TRJ8zxnTZN0SV3mPVaejFdJmu06Sx0bLGmVtXaNtfagpDmq3IYBw1r7obW2zPPjd5ISXOaphZr8jS9W5eNAqnxcnO65DwYUa+1ma2225/QeSUsltXebqs5cLCnDVvpOUlNjTFvXoY7idEmrrbW+nFipTllrv5S047CzD308VNcPZ0v6yFq7w1q7U9JHks7xRaaAL+AjaC9p4yE/5+nXD9AWknYd8qRa1TKunShpq7V2ZTWXW0kfGmOyjDHpfsx1rG72HD57qZrDODXZXoHkelXulVQlULdJTf7GPy/jeVwUqfJxErA8h8mTJX1fxcVDjTELjDHvG2P6+DVYzR3t/hJsjw2p8uhJdTsNwbBNftLaWrvZc3qLpNZVLFNn2yfSF1fiLWPMx5LaVHHRfdbaN/2dx1dquF7DdeS93xOstfnGmFaSPjLGLPO8kvOrI62LpEmSHlblE83Dqjykfr3/0tVcTbaJMeY+SWWSZlZzNQGxTcKBMaahpFcl3Wat3X3YxdmqPAS61zPu4A1J3fydsQZC6v7iGUdzkaR7q7g4WLbJr1hrrTHGrx8LCogCttaeUYtfy5fU4ZCfEzznHWq7Kg/nRHpe7Ve1TJ052noZYyIlXSYp9QjXke/5vs0Y87oqDzP6/cFb021kjJki6Z0qLqrJ9qpzNdgmv5F0gaTTrecNoCquIyC2SRVq8jf+aZk8z/2viSofJwHHGBOlyvKdaa197fDLDy1ka+17xpjnjTEtrbUBNSdxDe4vAfHYOAbnSsq21m49/IJg2SaH2GqMaWut3ew57L+timXyVfne9k8SVDkmyWvBfAj6LUnXeEZ1JqnyVdYPhy7geQL9TNIVnrNGSwqkPeozJC2z1uZVdaExpoExptFPp1U5SCi3qmVdOuz9qktVdcYfJXUzlaPSo1V5COstf+SrKWPMOZLulnSRtXZ/NcsE8japyd/4LVU+DqTKx8Wn1b3QcMnzvvSLkpZaa5+sZpk2P71/bYwZrMrns4B6MVHD+8tbkkZ5RkMPkVR0yGHRQFTtUbtg2CaHOfTxUF0/fCDpLGNMM8/ba2d5zvOe65FpR/tS5RN6nqQSSVslfXDIZfepctTncknnHnL+e5LaeU53VmUxr5L0iqQY1+t0SM7/SBp32HntJL13SPYFnq/FqjxM6jx3FesxXdIiSQtVeYdue/i6eH4+T5WjWVcH4rp47iMbJeV4vn4aLRw026Sqv7GkP6vyRYUkxXoeB6s8j4vOrjNXsx4nqPItjYWHbI/zJI376TEj6WbPNligykFzw1znrmI9qry/HLYeRtJEzzZbpEM+7RFoX5IaqLJQmxxyXlBsE1W+aNgsqdTTKTeocvzDJ5JWSvpYUnPPsmmSph7yu9d7HjOrJP3WV5mYCQsAAAeC+RA0AABBiwIGAMABChgAAAcoYAAAHKCAAQBwgAIGAMABChgAAAcoYAAAHPh/KE8tsxY0BTUAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer in PyTorch\n",
        "\n",
        "* torch.optim.SGD\n",
        "* torch.optim.Adadelta\n",
        "* torch.optim.Adagrad\n",
        "* torch.optim.Adam\n",
        "* torch.optim.SparseAdam\n",
        "* torch.optim.Adamax\n",
        "* torch.optim.ASGD\n",
        "* torch.optim.LBFGS\n",
        "* torch.optim.RMSprop\n",
        "* torch.optim.Rprop"
      ],
      "metadata": {
        "id": "zE3R7tOjuO_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code: mnist_softmax"
      ],
      "metadata": {
        "id": "CQNvl1gf877C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "metadata": {
        "id": "Ea2fzmXn9B5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "gJSqCsw29FWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "WIue7zM69GG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "metadata": {
        "id": "b3ubyPeq9KLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)"
      ],
      "metadata": {
        "id": "-Qr3APDB9NFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST data image of shape 28 * 28 = 784\n",
        "linear = torch.nn.Linear(784, 10, bias=True).to(device)"
      ],
      "metadata": {
        "id": "IgZUY8F49PSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(linear.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "ESsBZDk99RQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = linear(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXkhIDh29V3_",
        "outputId": "eae15cdf-ce8c-49fe-c898-dc7076ddd39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 0.614322782\n",
            "Epoch: 0002 cost = 0.344514906\n",
            "Epoch: 0003 cost = 0.308487236\n",
            "Epoch: 0004 cost = 0.291329682\n",
            "Epoch: 0005 cost = 0.281550318\n",
            "Epoch: 0006 cost = 0.274611473\n",
            "Epoch: 0007 cost = 0.269422829\n",
            "Epoch: 0008 cost = 0.264997125\n",
            "Epoch: 0009 cost = 0.261648208\n",
            "Epoch: 0010 cost = 0.259036928\n",
            "Epoch: 0011 cost = 0.256706297\n",
            "Epoch: 0012 cost = 0.254275411\n",
            "Epoch: 0013 cost = 0.252618015\n",
            "Epoch: 0014 cost = 0.251062483\n",
            "Epoch: 0015 cost = 0.249662459\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.targets.to(device)\n",
        "\n",
        "    prediction = linear(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = linear(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6yGGd9X9Z09",
        "outputId": "48dc08b4-6bb4-4710-8ac7-f4244daf89c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9071999788284302\n",
            "Label:  8\n",
            "Prediction:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code: mnist_nn"
      ],
      "metadata": {
        "id": "Xa-EHgmK-2aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "ZDoNANe3_vb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
        "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
        "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
        "relu = torch.nn.ReLU()"
      ],
      "metadata": {
        "id": "OXbHE1tX-63p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "torch.nn.init.normal_(linear1.weight)\n",
        "torch.nn.init.normal_(linear2.weight)\n",
        "torch.nn.init.normal_(linear3.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH12S85b_diu",
        "outputId": "8e1c16d3-467b-47bd-c16e-d6193884b243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.3417, -0.1432, -0.2168,  ..., -1.7705, -0.3518, -1.8839],\n",
              "        [-0.6825,  1.2642,  0.1248,  ...,  0.1173, -0.9166,  1.6033],\n",
              "        [ 0.7507, -1.1337, -0.6434,  ...,  0.7531,  0.3646,  0.8004],\n",
              "        ...,\n",
              "        [-0.1065, -2.5800, -1.2971,  ...,  0.2555, -0.4023,  0.7031],\n",
              "        [ 0.2406, -1.1735,  0.6788,  ..., -1.5424, -1.3025,  0.0885],\n",
              "        [-1.5290, -0.7568,  0.5967,  ...,  0.4376, -0.5958, -1.0516]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)"
      ],
      "metadata": {
        "id": "oBC1of8m-9R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "bflx_Ire-_aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r49cNVe_CWv",
        "outputId": "76a3dfd7-5f80-4129-98a5-81e67e56818e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 130.750518799\n",
            "Epoch: 0002 cost = 36.040737152\n",
            "Epoch: 0003 cost = 22.519535065\n",
            "Epoch: 0004 cost = 15.594206810\n",
            "Epoch: 0005 cost = 11.024921417\n",
            "Epoch: 0006 cost = 8.102440834\n",
            "Epoch: 0007 cost = 5.978103638\n",
            "Epoch: 0008 cost = 4.406246185\n",
            "Epoch: 0009 cost = 3.258172989\n",
            "Epoch: 0010 cost = 2.429973602\n",
            "Epoch: 0011 cost = 1.790950418\n",
            "Epoch: 0012 cost = 1.330271006\n",
            "Epoch: 0013 cost = 1.108717561\n",
            "Epoch: 0014 cost = 0.811608732\n",
            "Epoch: 0015 cost = 0.739445686\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.targets.to(device)\n",
        "\n",
        "    prediction = linear(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = linear(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2YAfLwa_HOe",
        "outputId": "abcc3866-6e8a-47af-8c37-0c5cd20c054c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9071999788284302\n",
            "Label:  5\n",
            "Prediction:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab-09-2 Weight initialization"
      ],
      "metadata": {
        "id": "yKIGsbYnvrJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습목표\n",
        "\n",
        "가중치 초기화(Weight Inititalization)에 대해 알아본다."
      ],
      "metadata": {
        "id": "JhLec45yvukE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 핵심키워드\n",
        "\n",
        "가중치 초기화(Weight Inititalization)  \n",
        "RBM inititalization  \n",
        "Xavier / He inititalization"
      ],
      "metadata": {
        "id": "LwknrXPxvxJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code: mnist_xavier"
      ],
      "metadata": {
        "id": "xsEtyETf_8QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
        "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
        "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "# xavier initialization\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crOVb6JD__qJ",
        "outputId": "bdf9fcc8-7c44-48b9-b864-5d688ba32c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0971,  0.0676,  0.1090,  ..., -0.1492, -0.0876, -0.0456],\n",
              "        [ 0.0282,  0.0288,  0.0284,  ..., -0.1209, -0.1224, -0.1065],\n",
              "        [ 0.0959, -0.1317,  0.0427,  ...,  0.1128,  0.0998, -0.0194],\n",
              "        ...,\n",
              "        [ 0.0776,  0.0072,  0.0894,  ...,  0.1459, -0.0689,  0.1469],\n",
              "        [ 0.1398,  0.0116, -0.0880,  ..., -0.0007,  0.1098, -0.0030],\n",
              "        [-0.0032, -0.0277,  0.0563,  ...,  0.0095, -0.0508,  0.0954]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofoRFpEoAaAb",
        "outputId": "4926482f-6e9b-4044-c196-19013e483b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 0.240632161\n",
            "Epoch: 0002 cost = 0.088111296\n",
            "Epoch: 0003 cost = 0.059663516\n",
            "Epoch: 0004 cost = 0.043071490\n",
            "Epoch: 0005 cost = 0.031818315\n",
            "Epoch: 0006 cost = 0.024266712\n",
            "Epoch: 0007 cost = 0.020237748\n",
            "Epoch: 0008 cost = 0.018892795\n",
            "Epoch: 0009 cost = 0.015171236\n",
            "Epoch: 0010 cost = 0.015256101\n",
            "Epoch: 0011 cost = 0.011704775\n",
            "Epoch: 0012 cost = 0.014042822\n",
            "Epoch: 0013 cost = 0.008679665\n",
            "Epoch: 0014 cost = 0.006597608\n",
            "Epoch: 0015 cost = 0.011575758\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "metadata": {
        "id": "jIqfBf65AgSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe691a2-cfa5-4ca0-c561-64ef0833b72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9776999950408936\n",
            "Label:  7\n",
            "Prediction:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab-09-3 Dropout"
      ],
      "metadata": {
        "id": "n71HrzUMyWg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습목표\n",
        "\n",
        "드롭아웃(Dropout) 에 대해 알아본다."
      ],
      "metadata": {
        "id": "JOfhmjOCyZH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 핵심키워드\n",
        "\n",
        "과최적화(Overfitting)  \n",
        "드롭아웃(Dropout) "
      ],
      "metadata": {
        "id": "tEYQn3hLybW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "keep_prob = 0.7\n",
        "\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)"
      ],
      "metadata": {
        "id": "uVH8E-_DBAxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
        "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "dropout = torch.nn.Dropout(p=1 - keep_prob)\n",
        "\n",
        "\n",
        "# xavier initialization\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "torch.nn.init.xavier_uniform_(linear4.weight)\n",
        "torch.nn.init.xavier_uniform_(linear5.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms3mIleIBEby",
        "outputId": "5692683a-a6f4-470a-f611-3d161f73fa8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.1013,  0.0053,  0.0971,  ..., -0.0665,  0.0287, -0.0446],\n",
              "        [-0.0769,  0.0265,  0.0179,  ...,  0.0110,  0.0256,  0.0362],\n",
              "        [ 0.0818,  0.0969,  0.0738,  ...,  0.0405, -0.1058, -0.0111],\n",
              "        ...,\n",
              "        [ 0.0956,  0.1014, -0.0257,  ..., -0.0362,  0.0723,  0.0043],\n",
              "        [-0.0504,  0.0369,  0.0684,  ...,  0.0905,  0.0754,  0.0589],\n",
              "        [-0.0697,  0.0652,  0.1005,  ..., -0.1008,  0.0636, -0.0476]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, dropout,\n",
        "                            linear2, relu, dropout,\n",
        "                            linear3, relu, dropout,\n",
        "                            linear4, relu, dropout,\n",
        "                            linear5).to(device)"
      ],
      "metadata": {
        "id": "x00SqT-EBFSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "model.train()    # set the model to train mode (dropout=True)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAa7Bg5hBPNM",
        "outputId": "1be145fc-93a1-4cef-e882-8564310497b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 0.307285279\n",
            "Epoch: 0002 cost = 0.144767687\n",
            "Epoch: 0003 cost = 0.110665329\n",
            "Epoch: 0004 cost = 0.094166458\n",
            "Epoch: 0005 cost = 0.084667444\n",
            "Epoch: 0006 cost = 0.074020423\n",
            "Epoch: 0007 cost = 0.069688194\n",
            "Epoch: 0008 cost = 0.062500820\n",
            "Epoch: 0009 cost = 0.057284005\n",
            "Epoch: 0010 cost = 0.054031365\n",
            "Epoch: 0011 cost = 0.050470892\n",
            "Epoch: 0012 cost = 0.049530797\n",
            "Epoch: 0013 cost = 0.047440074\n",
            "Epoch: 0014 cost = 0.046248231\n",
            "Epoch: 0015 cost = 0.043374743\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    model.eval()  # set the model to evaluation mode (dropout=False) \n",
        "\n",
        "    # Test the model using test sets\n",
        "    X_test = mnist_test.data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.targets.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.targets[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRYXFm-FBTb6",
        "outputId": "36761860-13c7-461c-c537-2b8d3ee1f5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9822999835014343\n",
            "Label:  9\n",
            "Prediction:  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab-09-4 Batch Normalization"
      ],
      "metadata": {
        "id": "LYqH5ewp0zl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습목표\n",
        "\n",
        "Batch Normalization 에 대해 알아본다."
      ],
      "metadata": {
        "id": "hSlHOsnv00q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 핵심키워드\n",
        "\n",
        "Batch Normalization  \n",
        "경사 소실(Gradient Vanishing) / 폭발(Exploding)"
      ],
      "metadata": {
        "id": "XEyDUF0k03Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code: mnist_batchnorm"
      ],
      "metadata": {
        "id": "h8uZtjWZ3wTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 10\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "bIfyF6qv00YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "metadata": {
        "id": "JMAEpf6mCFEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)"
      ],
      "metadata": {
        "id": "KuygzSHGCG59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "bn1 = torch.nn.BatchNorm1d(32)\n",
        "bn2 = torch.nn.BatchNorm1d(32)\n",
        "\n",
        "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "nn_linear3 = torch.nn.Linear(32, 10, bias=True)"
      ],
      "metadata": {
        "id": "m06nUhCQCIse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "bn_model = torch.nn.Sequential(linear1, relu, bn1,\n",
        "                            linear2, relu, bn2,\n",
        "                            linear3).to(device)\n",
        "nn_model = torch.nn.Sequential(nn_linear1, relu,\n",
        "                               nn_linear2, relu,\n",
        "                               nn_linear3).to(device)"
      ],
      "metadata": {
        "id": "e4lAywLICKkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n",
        "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Z95wDMVUCNkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Losses and Accuracies every epoch\n",
        "# We are going to plot them later\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "train_total_batch = len(train_loader)\n",
        "test_total_batch = len(test_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    bn_model.train()  # set the model to train mode\n",
        "\n",
        "    for X, Y in train_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        bn_optimizer.zero_grad()\n",
        "        bn_prediction = bn_model(X)\n",
        "        bn_loss = criterion(bn_prediction, Y)\n",
        "        bn_loss.backward()\n",
        "        bn_optimizer.step()\n",
        "\n",
        "        nn_optimizer.zero_grad()\n",
        "        nn_prediction = nn_model(X)\n",
        "        nn_loss = criterion(nn_prediction, Y)\n",
        "        nn_loss.backward()\n",
        "        nn_optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bn_model.eval()\n",
        "\n",
        "        # Test the model using train sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(train_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch\n",
        "\n",
        "        # Save train losses/acc\n",
        "        train_losses.append([bn_loss, nn_loss])\n",
        "        train_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        # Test the model using test sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(test_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\n",
        "\n",
        "        # Save valid losses/acc\n",
        "        valid_losses.append([bn_loss, nn_loss])\n",
        "        valid_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        print()\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "id": "ePDjzFjyCQ6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de09559-56a1-4b02-85b4-030b6e5f9bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.15844(bn_acc:0.95) vs No Batchnorm Loss(Acc): nn_loss:0.17941(nn_acc:0.95)\n",
            "[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.16411(bn_acc:0.95) vs No Batchnorm Loss(Acc): nn_loss:0.18746(nn_acc:0.95)\n",
            "\n",
            "[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.12180(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.15641(nn_acc:0.95)\n",
            "[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.13826(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.17910(nn_acc:0.95)\n",
            "\n",
            "[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.10577(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.16499(nn_acc:0.95)\n",
            "[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.12759(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.20435(nn_acc:0.95)\n",
            "\n",
            "[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08619(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.12469(nn_acc:0.96)\n",
            "[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.11456(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.17966(nn_acc:0.96)\n",
            "\n",
            "[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.09217(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.14464(nn_acc:0.96)\n",
            "[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.12007(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.19714(nn_acc:0.95)\n",
            "\n",
            "[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08240(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.12552(nn_acc:0.96)\n",
            "[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.11557(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.18492(nn_acc:0.95)\n",
            "\n",
            "[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07089(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.13707(nn_acc:0.96)\n",
            "[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.10903(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.19876(nn_acc:0.95)\n",
            "\n",
            "[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06293(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12429(nn_acc:0.97)\n",
            "[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.09493(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18946(nn_acc:0.95)\n",
            "\n",
            "[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06301(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.13278(nn_acc:0.96)\n",
            "[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.10361(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18579(nn_acc:0.95)\n",
            "\n",
            "[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05694(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.13454(nn_acc:0.96)\n",
            "[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.09615(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.21275(nn_acc:0.95)\n",
            "\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n",
        "    bn = [i[0] for i in loss_list]\n",
        "    nn = [i[1] for i in loss_list]\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.plot(bn, label='With BN')\n",
        "    plt.plot(nn, label='Without BN')\n",
        "    if ylim:\n",
        "        plt.ylim(ylim)\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid('on')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pr13LVjqCmEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_compare(train_losses, title='Training Loss at Epoch')\n",
        "plot_compare(train_accs, [0, 1.0], title='Training Acc at Epoch')\n",
        "plot_compare(valid_losses, title='Validation Loss at Epoch')\n",
        "plot_compare(valid_accs, [0, 1.0], title='Validation Acc at Epoch')"
      ],
      "metadata": {
        "id": "XmaA6_qzCpZ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d63dcb9-cae8-405f-9b06-acc4dda074a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'values'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-5b9d67bf9628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss at Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Acc at Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss at Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Acc at Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-227f0860563d>\u001b[0m in \u001b[0;36mplot_compare\u001b[0;34m(loss_list, ylim, title)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'With BN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Without BN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mylim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2761\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m   2763\u001b[0m         is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     '''\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}